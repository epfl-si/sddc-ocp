- tags: always
  include_vars: "{{ item }}"
  with_items:
    - versions.yml
    - resources.yml
    - software.yml
    - openshift-install-custom-manifests.yml

- name: ManagedCluster object (non-namespaced; creates namespace)
  kubernetes.core.k8s:
    definition:
      apiVersion: cluster.open-cluster-management.io/v1
      kind: ManagedCluster
      metadata:
        name: "{{ spoke_namespace_in_hub }}"
        labels:
          cloud: auto-detect
          vendor: auto-detect
      spec:
        hubAcceptsClient: true

- name: Wait for spoke namespace to be created
  retries: 6
  delay: 10
  register: _oc_get_spoke_namespace
  until: >-
    "namespace/%s" % spoke_namespace_in_hub == _oc_get_spoke_namespace.stdout
  changed_when: false
  shell:
    cmd: |
      oc get ns {{ spoke_namespace_in_hub }} -o name

- name: Pull secret for spoke (same as hub)
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: Secret
      type: kubernetes.io/dockerconfigjson
      metadata:
        name: spoke-pull-secret
        namespace: "{{ spoke_namespace_in_hub }}"
      stringData:
        .dockerconfigjson: "{{ xaasible_pull_secret }}"

# The following two objects are marked as required in `oc explain
# ClusterDeployment.spec.platform.vsphere`, but the details there are
# hazy or wrong. To find what these should look like, we had to take a
# look at what the clicky thing in the dashboard creates.
- name: vCenter CA certificate (Secret)
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      metadata:
        name: vcenter-cert-bundle
        namespace: "{{ spoke_namespace_in_hub }}"
      kind: Secret
      type: Opaque
      stringData:
        ca.crt: >-
          {{ lookup("vsphere_ca_bundle",
          "https://" + vsphere_credentials.host + "/certs/download.zip"
          ) }}

- name: vCenter credentials
  kubernetes.core.k8s:
    definition:
      kind: Secret
      metadata:
          name: spoke-vsphere-credentials
          namespace: "{{ spoke_namespace_in_hub }}"
      type: Opaque
      stringData:
        username: "{{ vsphere_credentials.user }}"
        password: "{{ vsphere_credentials.password }}"

- tags: always
  include_vars: "{{ item }}"
  with_items:
    # The `ocp-install-config.yaml` template in the next task below
    # wants these, so as to set up ssh access to managed nodes (and
    # also, https://github.com/ansible/ansible/issues/57751 forces us
    # to do things this way):
    - ssh-public-keys.yml
    - access.yml

- name: "`openshift-install-config` Secret"
  kubernetes.core.k8s:
    definition:
      kind: Secret
      metadata:
        name: openshift-install-config
        namespace: "{{ spoke_namespace_in_hub }}"
      stringData:
        "install-config.yaml": >-
          {{ lookup("template", "ocp-install-config.yaml") }}
  vars:
    # The template explicitly expects the following variables:
    ocp_config_pull_secret: "{{ xaasible_pull_secret }}"
    ocp_config_resources: >-
      {{ resources_openshift_clusters[inventory_hostname] }}

- name: ConfigMap for customized install manifests
  kubernetes.core.k8s:
    definition:
      kind: ConfigMap
      metadata:
        name: openshift-installer-custom-manifests
        namespace: "{{ spoke_namespace_in_hub }}"
      data: >-
        {{ openshift_install_custom_manifests }}

- name: ClusterDeployment object
  register: _spoke_clusterdeployment
  kubernetes.core.k8s:
    definition:
      apiVersion: hive.openshift.io/v1
      kind: ClusterDeployment
      metadata:
        # Just like the non-namespaced ManagedCluster object creates a
        # namespace with the same name, the Hive operator assumes that
        # ClusterDeployment objects must have the same name as the
        # namespace they live in for the “import cluster” feature to
        # work.
        name: "{{ spoke_namespace_in_hub }}"
        namespace: "{{ spoke_namespace_in_hub }}"
      spec:
        baseDomain: "{{ cluster_base_domain }}"
        clusterName: "{{ cluster_name }}"
        platform:
          vsphere:
            # As seen in `oc explain ClusterDeployment.spec.platform.vsphere`
            cluster:          "{{ _vsphere_placement.cluster }}"
            datacenter:       "{{ _vsphere_placement.dc }}"
            defaultDatastore: "{{ _vsphere_placement.datastore }}"
            folder: >-
              {{ lookup("template", "vm-folder-full",
              template_vars=dict(_vm_placement=_vsphere_placement))
              | trim }}
            network: "{{ _network_name }}"

            vCenter:  "{{ vsphere_credentials.host }}"
            # The objects referenced below are created by
            # ../../roles/openshift4-hub/tasks/hub-spokes-config.yml
            credentialsSecretRef:
              name: spoke-vsphere-credentials
            certificatesSecretRef:
              name: vcenter-cert-bundle
        provisioning:
          imageSetRef:
            name: "{{ spoke_clusterimageset_name }}"
          installConfigSecretRef:
            name: openshift-install-config
          manifestsConfigMapRef:
            name: openshift-installer-custom-manifests
            namespace: "{{ spoke_namespace_in_hub }}"
        pullSecretRef:
          name: spoke-pull-secret
  vars:
    _resources: >-
      {{ resources_openshift_clusters[inventory_hostname] }}
    _vsphere_placement: >-
      {{ _resources.vsphere_placement }}
    _network_name: >-
      {{ _vsphere_placement.network.name }}

- name: KlusterletAddonConfig (declares feature set for the hub-spoke registration)
  kubernetes.core.k8s:
    definition:
      apiVersion: agent.open-cluster-management.io/v1
      kind: KlusterletAddonConfig
      metadata:
        name: "{{ spoke_namespace_in_hub }}"
        namespace: "{{ spoke_namespace_in_hub }}"
      spec:
        applicationManager:
          enabled: true
        certPolicyController:
          enabled: true
        iamPolicyController:
          enabled: true
        policyController:
          enabled: true
        searchCollector:
          enabled: true

- run_once: true
  when: _spoke_clusterdeployment is changed
  pause:
    echo: false
    seconds: 1
    prompt: |
      ==========================================================================

      OpenShift installation is in progress in spoke clusters; this might take some time.

      If you are bored, feel free to watch progress at

          https://console-openshift-console.apps.{{ inventory_hub_cluster }}/multicloud/infrastructure/clusters/managed

      and click on the “Creating” hyperlinks to watch logs.

      ==========================================================================


- name: >-
    {{ "Wait for the spoke cluster(s) to come up"
    if _spoke_clusterdeployment is changed
    else "Check that the spoke cluster(s) are up"
    }}
  retries: >-
    {{ 50
    if _spoke_clusterdeployment is changed else 1 }}
  delay: 60
  register: _oc_get_clusterdeployment_installed
  until: >-
    "true" == _oc_get_clusterdeployment_installed.stdout
  changed_when: false
  shell:
    cmd: |
      oc -n {{ spoke_namespace_in_hub }} get clusterdeployment  {{ spoke_namespace_in_hub }} \
        -o jsonpath="{.spec.installed}"
